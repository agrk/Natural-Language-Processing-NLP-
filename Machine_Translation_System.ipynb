{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from keras) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from keras) (1.19.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pyyaml in c:\\users\\asile\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.1.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from keras) (1.19.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from keras) (1.19.3)\n",
      "Installing collected packages: h5py, keras\n",
      "Successfully installed h5py-3.1.0 keras-2.4.3\n"
     ]
    }
   ],
   "source": [
    "pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\asile\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.3)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.3)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.24.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.25.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (50.3.2.post20201202)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (50.3.2.post20201202)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.24.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.11.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.25.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\asile\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = 20000\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input: 20000\n",
      "num samples output: 20000\n",
      "num samples output input: 20000\n"
     ]
    }
   ],
   "source": [
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "\n",
    "count = 0\n",
    "for line in open(r'deu.txt', encoding=\"utf-8\"):\n",
    "    count += 1\n",
    "\n",
    "    if count > NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    input_sentence, output,_ = line.rstrip().split('\\t')\n",
    "\n",
    "    output_sentence = output + ' <eos>'\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(\"num samples input:\", len(input_sentences))\n",
    "print(\"num samples output:\", len(output_sentences))\n",
    "print(\"num samples output input:\", len(output_sentences_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go away.\n",
      "Mach ’ne Fliege! <eos>\n",
      "<sos> Mach ’ne Fliege!\n"
     ]
    }
   ],
   "source": [
    "print(input_sentences[172])\n",
    "print(output_sentences[172])\n",
    "print(output_sentences_inputs[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 3688\n",
      "Length of longest sentence in input: 6\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 8213\n",
      "Length of longest sentence in the output: 11\n"
     ]
    }
   ],
   "source": [
    "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (20000, 6)\n",
      "encoder_input_sequences[172]: [ 0  0  0  0 20 93]\n"
     ]
    }
   ],
   "source": [
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[172]:\", encoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_inputs[\"i'm\"])\n",
    "print(word2idx_inputs[\"walk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (20000, 11)\n",
      "decoder_input_sequences[172]: [   2   71 1346 1347    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "597\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_outputs[\"<sos>\"])\n",
    "print(word2idx_outputs[\"ich\"])\n",
    "print(word2idx_outputs[\"hörte\"])\n",
    "print(word2idx_outputs[\"auf.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open(r'glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = zeros((num_words, EMBEDDING_SIZE))\n",
    "for word, index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20725    0.84365   -0.076531   0.0071135 -0.13323    0.7837\n",
      " -0.491      0.94298   -0.2167    -0.74668    0.25125    0.62035\n",
      "  0.49758   -0.027116   0.27161    0.092964   0.38282    0.99705\n",
      " -0.79015   -0.27931    0.83799    0.59941   -0.20857   -0.40869\n",
      "  0.46501    0.47012   -1.1325    -0.57505    0.87657   -0.0051153\n",
      " -0.64612    0.21349    0.62074    0.31057    0.60625    0.64406\n",
      " -0.32349    0.1056     0.39087    0.46077    0.18394    0.10649\n",
      " -0.17585   -0.31971   -0.19235   -0.16422   -0.12337   -0.20295\n",
      "  0.26234   -0.072558   0.030499  -0.30478    0.068655   0.5949\n",
      "  0.25831   -2.111     -0.044159   0.66168    1.501      0.30353\n",
      " -0.24889    0.92395    0.010073  -0.24858    0.28253    0.26781\n",
      "  0.82791   -0.34017   -0.2274    -0.32975   -0.52374    0.0063988\n",
      " -0.17637   -1.1112     0.45905    0.24236   -0.25737   -0.24206\n",
      "  0.49061   -0.36632   -0.30224    0.41096   -0.46267    0.010152\n",
      " -0.74526   -0.016159   0.22009    0.015054   0.27948    0.27322\n",
      "  0.26239   -0.57978    0.022698  -0.55856   -0.21386   -0.097685\n",
      "  0.47189    0.32354    0.55492   -0.12433  ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dictionary[\"walk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56405002  0.13749     0.73861998 -0.51165998  0.13603     0.46792001\n",
      " -0.52835    -0.12946001  0.18187    -0.67803001 -0.51889998  0.37786999\n",
      "  0.76815999  1.01600003  0.37512001  0.31896001 -1.20899999  0.62366003\n",
      "  0.45614001 -0.50072998 -0.30096999  0.13631     0.50472999 -0.90983999\n",
      "  1.35839999  1.32650006 -0.12594    -0.23116     0.87313002  0.11791\n",
      "  1.01129997 -0.22823     0.64191997  0.32475999 -0.062599    0.42541\n",
      " -0.29337999  0.44635999  0.85812002 -0.13944     0.52234    -0.72368997\n",
      " -0.13501    -0.26104    -0.45321    -0.43656999  0.29662001  0.25174001\n",
      " -0.24169999 -0.72171003 -0.47907001 -0.10003     0.17557999  0.51245999\n",
      " -0.21233    -0.52855998  0.227      -0.084546    0.72983003 -0.041315\n",
      "  1.03250003  1.70200002 -0.17155001  0.55111003  0.06694    -0.60178\n",
      " -0.28736001  0.16134     0.32778999 -0.59762001  0.085608    0.31759\n",
      "  0.64714003 -0.26975    -0.25167999 -0.46195    -0.4201      0.51457\n",
      "  0.14984     0.089569    0.41067001 -0.27408001 -0.78237998 -0.10916\n",
      " -1.38119996 -0.37836999 -0.16368     0.74410999  0.059538    0.83056998\n",
      "  0.21916001  0.59745002  0.057972   -0.13381    -0.53156    -0.06659\n",
      " -0.92752999  0.13383999  0.2597     -0.15569   ]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[539])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(input_sentences),\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 11, 8214)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(decoder_output_sequences):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs_placeholder,\n",
    "  decoder_inputs_placeholder], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "282/282 [==============================] - 59s 195ms/step - loss: 2.8060 - accuracy: 0.6478 - val_loss: 1.8894 - val_accuracy: 0.7411\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 60s 212ms/step - loss: 1.6680 - accuracy: 0.7568 - val_loss: 1.6622 - val_accuracy: 0.7694\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 61s 216ms/step - loss: 1.4276 - accuracy: 0.7854 - val_loss: 1.5297 - val_accuracy: 0.7883\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 63s 222ms/step - loss: 1.2840 - accuracy: 0.8041 - val_loss: 1.4588 - val_accuracy: 0.7980\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 66s 234ms/step - loss: 1.1793 - accuracy: 0.8176 - val_loss: 1.4180 - val_accuracy: 0.8075\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(\n",
    "    [encoder_input_sequences, decoder_input_sequences],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = idx2word_target[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: She is young.\n",
      "Response: sie ist jung.\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_sentences))\n",
    "input_seq = encoder_input_sequences[i:i+1]\n",
    "translation = translate_sentence(input_seq)\n",
    "print('-')\n",
    "print('Input:', input_sentences[i])\n",
    "print('Response:', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
